{"cells":[{"cell_type":"markdown","source":["## Subword Tokenization and Dataset Preparation\n","\n","After completing the preprocessing stage, the next step involves preparing the dataset for Neural Machine Translation by applying **subword segmentation**.  \n","Subwording helps the model handle **rare words**, **morphological variations**, and **unknown tokens** by breaking text into smaller, learnable units.\n","\n"," *By the end of this stage, the dataset is completely subworded, properly split, and organized — ready for model training in the next phase.*\n"],"metadata":{"id":"uxrYA0lIcjSZ"},"id":"uxrYA0lIcjSZ"},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AQP9KlGZvwm-","executionInfo":{"status":"ok","timestamp":1761418908786,"user_tz":-330,"elapsed":18790,"user":{"displayName":"Prashanth Samkumar","userId":"14640335479218702548"}},"outputId":"62dd00f8-6f2c-435d-92ac-20f703264f07"},"id":"AQP9KlGZvwm-","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Colab Notebooks/LLM/workflow/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oh3mm79Gv9JJ","executionInfo":{"status":"ok","timestamp":1761418926717,"user_tz":-330,"elapsed":605,"user":{"displayName":"Prashanth Samkumar","userId":"14640335479218702548"}},"outputId":"28ced7b0-6a85-4c53-afbf-1e34c7a1659a"},"id":"oh3mm79Gv9JJ","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/LLM/workflow\n"]}]},{"cell_type":"markdown","source":["### Step 1: Setting Up the Environment\n","  A new working directory is created, and the **MT-Preparation** repository from GitHub is cloned.  \n","This repository provides standardized scripts for performing subword tokenization, data cleaning, and dataset splitting.  \n","It ensures consistent preprocessing across different language pairs and simplifies the subwording process.\n"],"metadata":{"id":"9r7cPAr5ctze"},"id":"9r7cPAr5ctze"},{"cell_type":"code","execution_count":null,"id":"b4a97bd1","metadata":{"collapsed":true,"id":"b4a97bd1","outputId":"531f5a95-2f44-450f-a4bb-a9d1fd7493de"},"outputs":[{"name":"stdout","output_type":"stream","text":["/home/prashanth/project/NMT/nmt\n","Cloning into 'MT-Preparation'...\n","remote: Enumerating objects: 323, done.\u001b[K\n","remote: Counting objects: 100% (55/55), done.\u001b[K\n","remote: Compressing objects: 100% (35/35), done.\u001b[K\n","remote: Total 323 (delta 35), reused 21 (delta 20), pack-reused 268 (from 2)\u001b[K\n","Receiving objects: 100% (323/323), 94.95 KiB | 382.00 KiB/s, done.\n","Resolving deltas: 100% (156/156), done.\n"]}],"source":["\n","# Create a directory and clone the Github MT-Preparation repository\n","!mkdir -p nmt\n","%cd nmt\n","!git clone https://github.com/ymoslem/MT-Preparation.git"]},{"cell_type":"code","execution_count":null,"id":"21f7ec37","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"21f7ec37","executionInfo":{"status":"ok","timestamp":1761418954040,"user_tz":-330,"elapsed":9297,"user":{"displayName":"Prashanth Samkumar","userId":"14640335479218702548"}},"outputId":"279abbd6-e322-4401-9d58-06af3f5d70ad"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from -r MT-Preparation/requirements.txt (line 1)) (2.0.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from -r MT-Preparation/requirements.txt (line 2)) (2.2.2)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from -r MT-Preparation/requirements.txt (line 3)) (0.2.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->-r MT-Preparation/requirements.txt (line 2)) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->-r MT-Preparation/requirements.txt (line 2)) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->-r MT-Preparation/requirements.txt (line 2)) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->-r MT-Preparation/requirements.txt (line 2)) (1.17.0)\n"]}],"source":["# Install the requirements from the cloned repository\n","!pip3 install -r MT-Preparation/requirements.txt"]},{"cell_type":"markdown","source":["\n","### Step 2: Training the Subword Model\n","We train a **SentencePiece unigram model** using both the English and Telugu parallel data.  \n","During training:\n","- The model learns the most frequent and meaningful subword units in both languages.  \n","- Multilingual tags (e.g., `<2te-Computer_science>`, `<2en-Mathematics>`) are included to make sure they are treated as complete tokens.  \n","- The output of this step is two model files — one for the source language (English) and one for the target language (Telugu).\n","\n","This step is crucial because it defines how words and subwords will be represented during model training and inference."],"metadata":{"id":"ickxKivmc5tr"},"id":"ickxKivmc5tr"},{"cell_type":"code","execution_count":null,"id":"b2e40b5b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"b2e40b5b","executionInfo":{"status":"ok","timestamp":1761419050793,"user_tz":-330,"elapsed":55564,"user":{"displayName":"Prashanth Samkumar","userId":"14640335479218702548"}},"outputId":"6e969d70-d8b3-431a-be4c-706c974da012"},"outputs":[{"output_type":"stream","name":"stdout","text":["['<2te-Computer_science>', ' <2te-Mathematics>', ' <2en-Computer_science>', ' <2en-Mathematics>']\n","sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n","trainer_spec {\n","  input: dataset/parallel_copora/en-te.en\n","  input_format: \n","  model_prefix: source\n","  model_type: UNIGRAM\n","  vocab_size: 50000\n","  self_test_sample_size: 0\n","  character_coverage: 0.9995\n","  input_sentence_size: 0\n","  shuffle_input_sentence: 1\n","  seed_sentencepiece_size: 1000000\n","  shrinking_factor: 0.75\n","  max_sentence_length: 4192\n","  num_threads: 16\n","  num_sub_iterations: 2\n","  max_sentencepiece_length: 16\n","  split_by_unicode_script: 1\n","  split_by_number: 1\n","  split_by_whitespace: 1\n","  split_digits: 1\n","  pretokenization_delimiter: \n","  treat_whitespace_as_suffix: 0\n","  allow_whitespace_only_pieces: 0\n","  user_defined_symbols: <2te-Computer_science>\n","  user_defined_symbols:  <2te-Mathematics>\n","  user_defined_symbols:  <2en-Computer_science>\n","  user_defined_symbols:  <2en-Mathematics>\n","  required_chars: \n","  byte_fallback: 0\n","  vocabulary_output_piece_score: 1\n","  train_extremely_large_corpus: 0\n","  seed_sentencepieces_file: \n","  hard_vocab_limit: 0\n","  use_all_vocab: 0\n","  unk_id: 0\n","  bos_id: 1\n","  eos_id: 2\n","  pad_id: -1\n","  unk_piece: <unk>\n","  bos_piece: <s>\n","  eos_piece: </s>\n","  pad_piece: <pad>\n","  unk_surface:  ⁇ \n","  enable_differential_privacy: 0\n","  differential_privacy_noise_level: 0\n","  differential_privacy_clipping_threshold: 0\n","}\n","normalizer_spec {\n","  name: nmt_nfkc\n","  add_dummy_prefix: 1\n","  remove_extra_whitespaces: 1\n","  escape_whitespaces: 1\n","  normalization_rule_tsv: \n","}\n","denormalizer_spec {}\n","trainer_interface.cc(355) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n","trainer_interface.cc(186) LOG(INFO) Loading corpus: dataset/parallel_copora/en-te.en\n","trainer_interface.cc(382) LOG(WARNING) Found too long line (9283 > 4192).\n","trainer_interface.cc(384) LOG(WARNING) Too long lines are skipped in the training.\n","trainer_interface.cc(385) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n","trainer_interface.cc(411) LOG(INFO) Loaded all 133725 sentences\n","trainer_interface.cc(418) LOG(INFO) Skipped 17 too long sentences.\n","trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <unk>\n","trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <s>\n","trainer_interface.cc(427) LOG(INFO) Adding meta_piece: </s>\n","trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <2te-Computer_science>\n","trainer_interface.cc(427) LOG(INFO) Adding meta_piece:  <2te-Mathematics>\n","trainer_interface.cc(427) LOG(INFO) Adding meta_piece:  <2en-Computer_science>\n","trainer_interface.cc(427) LOG(INFO) Adding meta_piece:  <2en-Mathematics>\n","trainer_interface.cc(432) LOG(INFO) Normalizing sentences...\n","trainer_interface.cc(541) LOG(INFO) all chars count=24870681\n","trainer_interface.cc(552) LOG(INFO) Done: 99.9506% characters are covered.\n","trainer_interface.cc(562) LOG(INFO) Alphabet size=123\n","trainer_interface.cc(563) LOG(INFO) Final character coverage=0.999506\n","trainer_interface.cc(594) LOG(INFO) Done! preprocessed 133725 sentences.\n","unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n","unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=17266584\n","unigram_model_trainer.cc(312) LOG(INFO) Initialized 103505 seed sentencepieces\n","trainer_interface.cc(600) LOG(INFO) Tokenizing input sentences with whitespace: 133725\n","trainer_interface.cc(611) LOG(INFO) Done! 100899\n","unigram_model_trainer.cc(602) LOG(INFO) Using 100899 sentences for EM training\n","unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=42200 obj=11.5299 num_tokens=281131 num_tokens/piece=6.66187\n","unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=33064 obj=8.9636 num_tokens=280899 num_tokens/piece=8.49561\n","trainer_interface.cc(689) LOG(INFO) Saving model: source.model\n","trainer_interface.cc(701) LOG(INFO) Saving vocabs: source.vocab\n","trainer_interface.cc(709) LOG(WARNING) The piece [ <2te-Mathematics>] contains escaped characters that break the format of source.vocab\n","trainer_interface.cc(709) LOG(WARNING) The piece [ <2en-Computer_science>] contains escaped characters that break the format of source.vocab\n","trainer_interface.cc(709) LOG(WARNING) The piece [ <2en-Mathematics>] contains escaped characters that break the format of source.vocab\n","Done, training a SentencePiece model for the Source finished successfully!\n","sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n","trainer_spec {\n","  input: dataset/parallel_copora/en-te.te\n","  input_format: \n","  model_prefix: target\n","  model_type: UNIGRAM\n","  vocab_size: 50000\n","  self_test_sample_size: 0\n","  character_coverage: 0.9995\n","  input_sentence_size: 0\n","  shuffle_input_sentence: 1\n","  seed_sentencepiece_size: 1000000\n","  shrinking_factor: 0.75\n","  max_sentence_length: 4192\n","  num_threads: 16\n","  num_sub_iterations: 2\n","  max_sentencepiece_length: 16\n","  split_by_unicode_script: 1\n","  split_by_number: 1\n","  split_by_whitespace: 1\n","  split_digits: 1\n","  pretokenization_delimiter: \n","  treat_whitespace_as_suffix: 0\n","  allow_whitespace_only_pieces: 0\n","  user_defined_symbols: <2te-Computer_science>\n","  user_defined_symbols:  <2te-Mathematics>\n","  user_defined_symbols:  <2en-Computer_science>\n","  user_defined_symbols:  <2en-Mathematics>\n","  required_chars: \n","  byte_fallback: 0\n","  vocabulary_output_piece_score: 1\n","  train_extremely_large_corpus: 0\n","  seed_sentencepieces_file: \n","  hard_vocab_limit: 0\n","  use_all_vocab: 0\n","  unk_id: 0\n","  bos_id: 1\n","  eos_id: 2\n","  pad_id: -1\n","  unk_piece: <unk>\n","  bos_piece: <s>\n","  eos_piece: </s>\n","  pad_piece: <pad>\n","  unk_surface:  ⁇ \n","  enable_differential_privacy: 0\n","  differential_privacy_noise_level: 0\n","  differential_privacy_clipping_threshold: 0\n","}\n","normalizer_spec {\n","  name: nmt_nfkc\n","  add_dummy_prefix: 1\n","  remove_extra_whitespaces: 1\n","  escape_whitespaces: 1\n","  normalization_rule_tsv: \n","}\n","denormalizer_spec {}\n","trainer_interface.cc(355) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n","trainer_interface.cc(186) LOG(INFO) Loading corpus: dataset/parallel_copora/en-te.te\n","trainer_interface.cc(382) LOG(WARNING) Found too long line (5680 > 4192).\n","trainer_interface.cc(384) LOG(WARNING) Too long lines are skipped in the training.\n","trainer_interface.cc(385) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n","trainer_interface.cc(411) LOG(INFO) Loaded all 133509 sentences\n","trainer_interface.cc(418) LOG(INFO) Skipped 233 too long sentences.\n","trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <unk>\n","trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <s>\n","trainer_interface.cc(427) LOG(INFO) Adding meta_piece: </s>\n","trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <2te-Computer_science>\n","trainer_interface.cc(427) LOG(INFO) Adding meta_piece:  <2te-Mathematics>\n","trainer_interface.cc(427) LOG(INFO) Adding meta_piece:  <2en-Computer_science>\n","trainer_interface.cc(427) LOG(INFO) Adding meta_piece:  <2en-Mathematics>\n","trainer_interface.cc(432) LOG(INFO) Normalizing sentences...\n","trainer_interface.cc(541) LOG(INFO) all chars count=23790849\n","trainer_interface.cc(552) LOG(INFO) Done: 99.951% characters are covered.\n","trainer_interface.cc(562) LOG(INFO) Alphabet size=230\n","trainer_interface.cc(563) LOG(INFO) Final character coverage=0.99951\n","trainer_interface.cc(594) LOG(INFO) Done! preprocessed 133509 sentences.\n","unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n","unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=15356093\n","unigram_model_trainer.cc(312) LOG(INFO) Initialized 262764 seed sentencepieces\n","trainer_interface.cc(600) LOG(INFO) Tokenizing input sentences with whitespace: 133509\n","trainer_interface.cc(611) LOG(INFO) Done! 175227\n","unigram_model_trainer.cc(602) LOG(INFO) Using 175227 sentences for EM training\n","unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=76989 obj=13.2996 num_tokens=469398 num_tokens/piece=6.09695\n","unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=63441 obj=10.2647 num_tokens=470532 num_tokens/piece=7.41684\n","unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=54971 obj=10.2319 num_tokens=474819 num_tokens/piece=8.63763\n","unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=54790 obj=10.2217 num_tokens=475484 num_tokens/piece=8.6783\n","trainer_interface.cc(689) LOG(INFO) Saving model: target.model\n","trainer_interface.cc(701) LOG(INFO) Saving vocabs: target.vocab\n","trainer_interface.cc(709) LOG(WARNING) The piece [ <2te-Mathematics>] contains escaped characters that break the format of target.vocab\n","trainer_interface.cc(709) LOG(WARNING) The piece [ <2en-Computer_science>] contains escaped characters that break the format of target.vocab\n","trainer_interface.cc(709) LOG(WARNING) The piece [ <2en-Mathematics>] contains escaped characters that break the format of target.vocab\n","Done, training a SentencePiece model for the Target finished successfully!\n"]}],"source":["# Train a SentencePiece model for subword tokenization\n","!python3 MT-Preparation/subwording/1-train_unigram.py dataset/parallel_copora/en-te.en dataset/parallel_copora/en-te.te \\\n","    \"<2te-Computer_science>, <2te-Mathematics>, <2en-Computer_science>, <2en-Mathematics>\""]},{"cell_type":"markdown","source":["### Step 3: Applying Subword Tokenization\n","Once the models are trained, the raw parallel sentences are tokenized into subword units.  \n","Each sentence is segmented according to the learned vocabulary, producing parallel subworded files such as:\n","- English source file (e.g., `en-te.en.subword`)  \n","- Telugu target file (e.g., `en-te.te.subword`)  \n","\n","These subworded files are saved in a structured directory, ready to be used for model training."],"metadata":{"id":"PcMBjBI9c_W4"},"id":"PcMBjBI9c_W4"},{"cell_type":"code","execution_count":null,"id":"6a2aca34","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"6a2aca34","executionInfo":{"status":"ok","timestamp":1761419111762,"user_tz":-330,"elapsed":24596,"user":{"displayName":"Prashanth Samkumar","userId":"14640335479218702548"}},"outputId":"5883d60a-5648-4ef7-c053-2c8b8fb5ea4c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Source Model: source.model\n","Target Model: target.model\n","Source Dataset: dataset/parallel_copora/en-te.en\n","Target Dataset: dataset/parallel_copora/en-te.te\n","Done subwording the source file! Output: dataset/parallel_copora/en-te.en.subword\n","Done subwording the target file! Output: dataset/parallel_copora/en-te.te.subword\n"]}],"source":["# Subword the dataset\n","!python3 MT-Preparation/subwording/2-subword.py source.model target.model dataset/parallel_copora/en-te.en dataset/parallel_copora/en-te.te"]},{"cell_type":"markdown","source":["###  Step 4: Splitting the Dataset\n","The tokenized dataset is divided into **training**, **development (validation)**, and **testing** subsets.  \n","A fixed number of segments (in this case, 2000 each) are reserved for development and testing to ensure balanced evaluation.  \n","The remaining data is used for training the NMT model.\n","\n","This ensures that:\n","- The model is trained on a large, diverse dataset.  \n","- Performance is validated and tested on unseen data for fair evaluation."],"metadata":{"id":"dN3c6xpydBKC"},"id":"dN3c6xpydBKC"},{"cell_type":"code","execution_count":null,"id":"527c292b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"527c292b","executionInfo":{"status":"ok","timestamp":1761419461089,"user_tz":-330,"elapsed":7655,"user":{"displayName":"Prashanth Samkumar","userId":"14640335479218702548"}},"outputId":"30fe2f48-975e-4060-c7b4-82b109b25beb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Dataframe shape: (133742, 2)\n","--- Empty Cells Deleted --> Rows: 133742\n","--- Wrote Files\n","Done!\n","Output files\n","dataset/subword_corpora/en-te.en.subword.train\n","dataset/subword_corpora/en-te.te.subword.train\n","dataset/subword_corpora/en-te.en.subword.dev\n","dataset/subword_corpora/en-te.te.subword.dev\n","dataset/subword_corpora/en-te.en.subword.test\n","dataset/subword_corpora/en-te.te.subword.test\n"]}],"source":["# Split the dataset into training set, development set, and test set\n","# Development and test sets should be between 1000 and 5000 segments (here we chose 2000)\n","!python3 MT-Preparation/train_dev_split/train_dev_test_split.py 2000 2000  dataset/subword_corpora/en-te.en.subword dataset/subword_corpora/en-te.te.subword"]},{"cell_type":"code","execution_count":null,"id":"9a66fd33","metadata":{"id":"9a66fd33"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"NMT","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}